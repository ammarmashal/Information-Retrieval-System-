{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd797f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GEN AI Projects\\genaip39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# With these updated imports\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "#from langchain_community.embeddings import GooglePalmEmbeddings\n",
    "#from langchain_community.llms import GooglePalm\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4310097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import get_pdf_text, get_text_chunks, get_vector_store, get_conversational_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f5b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunks: ['Deep learning is a subfield of machine learning. It uses neural networks.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Deep learning is a subfield of machine learning. It uses neural networks.\"\n",
    "chunks = get_text_chunks(text)\n",
    "print(\" Chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e79d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store: <langchain_community.vectorstores.faiss.FAISS object at 0x000001B9EA0E2AC0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\gen ai projects\\information-retrieval-system-\\src\\helper.py:62: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain: memory=ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), output_key='answer', return_messages=True, memory_key='chat_history') verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n    You are an expert document analysis assistant. Follow these guidelines:\\n\\n    1. Answer strictly based on the provided context\\n    2. If unsure, say \"يا عم اسأل سؤال في الفايل مش عاوزين وجع دماغ \"\\n    3. Structure your response:\\n        - Direct answer first\\n        - Supporting evidence from document\\n        - Page reference if available\\n\\n    Context: {context}\\n    Question: {question}\\n\\n    Provide a concise, accurate response:\\n    '), llm=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash-latest', google_api_key=SecretStr('**********'), temperature=0.2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001B9FA468880>, default_metadata=(), convert_system_message_to_human=True), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash-latest', google_api_key=SecretStr('**********'), temperature=0.2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001B9FA468880>, default_metadata=(), convert_system_message_to_human=True), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B9EA0E2AC0>, search_kwargs={'k': 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ammar\\AppData\\Local\\Temp\\ipykernel_296\\3749063264.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain({\"question\": \"What is deep learning?\", \"chat_history\": []})\n",
      "d:\\GEN AI Projects\\genaip39\\lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subfield of machine learning that uses neural networks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "store = get_vector_store(chunks)\n",
    "print(\"Vector Store:\", store)\n",
    "chain = get_conversational_chain(store)\n",
    "print(\"Chain:\", chain)\n",
    "response = chain({\"question\": \"What is deep learning?\", \"chat_history\": []})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6c9d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeautifulSoup is installed and working!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "print(\"BeautifulSoup is installed and working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit run app.py --server.port 8502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2deb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['models/chat-bison-001']\n",
      "['models/text-bison-001']\n",
      "['models/embedding-gecko-001']\n",
      "['models/gemini-1.0-pro-vision-latest']\n",
      "['models/gemini-pro-vision']\n",
      "['models/gemini-1.5-pro-latest']\n",
      "['models/gemini-1.5-pro-001']\n",
      "['models/gemini-1.5-pro-002']\n",
      "['models/gemini-1.5-pro']\n",
      "['models/gemini-1.5-flash-latest']\n",
      "['models/gemini-1.5-flash-001']\n",
      "['models/gemini-1.5-flash-001-tuning']\n",
      "['models/gemini-1.5-flash']\n",
      "['models/gemini-1.5-flash-002']\n",
      "['models/gemini-1.5-flash-8b']\n",
      "['models/gemini-1.5-flash-8b-001']\n",
      "['models/gemini-1.5-flash-8b-latest']\n",
      "['models/gemini-1.5-flash-8b-exp-0827']\n",
      "['models/gemini-1.5-flash-8b-exp-0924']\n",
      "['models/gemini-2.5-pro-exp-03-25']\n",
      "['models/gemini-2.5-pro-preview-03-25']\n",
      "['models/gemini-2.0-flash-exp']\n",
      "['models/gemini-2.0-flash']\n",
      "['models/gemini-2.0-flash-001']\n",
      "['models/gemini-2.0-flash-exp-image-generation']\n",
      "['models/gemini-2.0-flash-lite-001']\n",
      "['models/gemini-2.0-flash-lite']\n",
      "['models/gemini-2.0-flash-lite-preview-02-05']\n",
      "['models/gemini-2.0-flash-lite-preview']\n",
      "['models/gemini-2.0-pro-exp']\n",
      "['models/gemini-2.0-pro-exp-02-05']\n",
      "['models/gemini-exp-1206']\n",
      "['models/gemini-2.0-flash-thinking-exp-01-21']\n",
      "['models/gemini-2.0-flash-thinking-exp']\n",
      "['models/gemini-2.0-flash-thinking-exp-1219']\n",
      "['models/learnlm-1.5-pro-experimental']\n",
      "['models/gemma-3-1b-it']\n",
      "['models/gemma-3-4b-it']\n",
      "['models/gemma-3-12b-it']\n",
      "['models/gemma-3-27b-it']\n",
      "['models/embedding-001']\n",
      "['models/text-embedding-004']\n",
      "['models/gemini-embedding-exp-03-07']\n",
      "['models/gemini-embedding-exp']\n",
      "['models/aqa']\n",
      "['models/imagen-3.0-generate-002']\n",
      "['models/gemini-2.0-flash-live-001']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.generativeai import list_models\n",
    "\n",
    "for model in list_models():\n",
    "    print([model.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2fdeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")  # Updated model\n",
    "response = model.generate_content(\"Hello\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03de084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ammar\\AppData\\Local\\Temp\\ipykernel_296\\2452428506.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Explicitly set the base URL (even if default)\n",
    "llm = Ollama(\n",
    "    model=\"llama3\", \n",
    "    base_url=\"http://localhost:11434\",  # ← Critical for Windows\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd6517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:11434\")\n",
    "print(response.status_code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bacdc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key: AIzaSyCmOAG6ntimu7d3waa9a6Y8o0jZPhzyrIs\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read the Google API key\n",
    "GOOGLE_API_KEY  = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(\"Google API Key:\", GOOGLE_API_KEY )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f98acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
